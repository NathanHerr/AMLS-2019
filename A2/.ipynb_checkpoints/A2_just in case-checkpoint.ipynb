{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from natsort import natsorted, ns\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of image names and corresponding emotion classifications\n",
    "image_dic = pd.read_excel('../Datasets/labels_A.xlsx')\n",
    "image_dic = image_dic[['img_name.jpg', 'smiling']] # Choose columns which are of importance\n",
    "df = pd.DataFrame(image_dic)\n",
    "df.to_excel('../Datasets/source_emotions/labels_A2.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate smiling and not_smiling images and corresponding labels into folders\n",
    "\n",
    "source_emotions = pd.read_excel('../Datasets/source_emotions/labels_A2.xlsx')\n",
    "source_images_file_paths = glob.glob (\"../Datasets/source_images/*.jpg\") #find all paths which match the given path\n",
    "source_images_file_paths = natsorted(source_images_file_paths) #sort the list of file names such that the image list will be in the correct order\n",
    "\n",
    "smiling_images = []\n",
    "not_smiling_images = []\n",
    "\n",
    "smiling_directory = \"../Datasets/sorted_sets/smiling/\"\n",
    "not_smiling_directory = \"../Datasets/sorted_sets/not_smiling/\"\n",
    "\n",
    "for file_path in source_images_file_paths:\n",
    "    image = cv2.imread(file_path, cv2.COLOR_RGB2BGR) #read the image\n",
    "    image_name = os.path.basename(file_path)\n",
    "    image_label = source_emotions[source_emotions['img_name.jpg']==image_name]['smiling'].iloc[0]\n",
    "    if(image_label == 1):\n",
    "        smiling_images.append(image)\n",
    "        directory = ''.join([smiling_directory,os.path.basename(image_name)])\n",
    "        cv2.imwrite(directory, image)\n",
    "    else:\n",
    "        not_smiling_images.append(image)\n",
    "        directory = ''.join([not_smiling_directory,os.path.basename(image_name)])\n",
    "        cv2.imwrite(directory, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deetcting faces and cropping\n",
    "\n",
    "faceDet = cv2.CascadeClassifier(\"../OpenCV_FaceCascade/haarcascade_frontalface_default.xml\")\n",
    "faceDet_two = cv2.CascadeClassifier(\"../OpenCV_FaceCascade/haarcascade_frontalface_alt2.xml\")\n",
    "faceDet_three = cv2.CascadeClassifier(\"../OpenCV_FaceCascade/haarcascade_frontalface_alt.xml\")\n",
    "faceDet_four = cv2.CascadeClassifier(\"../OpenCV_FaceCascade/haarcascade_frontalface_alt_tree.xml\")\n",
    "emotions = [\"smiling\", \"not_smiling\"] #Define emotions\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(\"../deep-learning-face-detection/deploy.prototxt.txt\", \"../deep-learning-face-detection/res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "\n",
    "def DNN_Face_Detection(emotion):\n",
    "    files = glob.glob(\"../Datasets/sorted_sets/%s/*.jpg\" %emotion) #Get list of all images with emotion\n",
    "    filenumber = 0\n",
    "    for f in files:\n",
    "        # load the input image and construct an input blob for the image\n",
    "        # by resizing to a fixed 300x300 pixels and then normalizing it\n",
    "        image = cv2.imread(f)\n",
    "        (h, w) = image.shape[:2]\n",
    "        #blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "        \n",
    "        BGR_ave = image.mean(axis=(0,1))\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (BGR_ave[2], BGR_ave[1], BGR_ave[0]))\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "\n",
    "        actual_detections = detections[0][0]\n",
    "        detection_accuracies = detections[0][0][:,2]\n",
    "\n",
    "        index_values = np.argsort(detection_accuracies)\n",
    "        index_values = index_values[::-1]\n",
    "\n",
    "        sorted_detections = [actual_detections[i] for i in index_values]\n",
    "        sorted_detections = np.asarray(sorted_detections)\n",
    "\n",
    "        confidence = sorted_detections[0, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the `confidence` is greater than the minimum confidence\n",
    "        if confidence > 0.9:\n",
    "            # compute the (x, y)-coordinates of the bounding box for the object\n",
    "            box = sorted_detections[0, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            cropped = image[startY:endY, startX:endX] #Cut the frame to size\n",
    "            try:\n",
    "                out = cv2.resize(cropped, (300, 300)) #Resize face so all images have same size\n",
    "                cv2.imwrite(\"../Datasets/A2_dataset_DNN/%s/%s.jpg\" %(emotion, filenumber), out) #Write image - don' need to worry about keeping track of labels associated because already sorted\n",
    "                filenumber += 1 #Increment image number\n",
    "            except:\n",
    "                pass #If error, pass file\n",
    "                    \n",
    "def detect_face_DNN(emotion):\n",
    "    files = glob.glob(\"../Datasets/sorted_sets/%s/*.jpg\" %emotion) #Get list of all images with emotion\n",
    "    filenumber = 0\n",
    "    for f in files:\n",
    "        # load the input image and construct an input blob for the image\n",
    "        # by resizing to a fixed 300x300 pixels and then normalizing it\n",
    "        image = cv2.imread(f)\n",
    "        (h, w) = image.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        print(detections.shape[2])\n",
    "        # loop over the detections\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            # extract the confidence (i.e., probability) associated with the\n",
    "            # prediction\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            # filter out weak detections by ensuring the `confidence` is greater than the minimum confidence\n",
    "            if confidence > 0.999:\n",
    "                # compute the (x, y)-coordinates of the bounding box for the object\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                cropped = image[startY:endY, startX:endX] #Cut the frame to size\n",
    "                try:\n",
    "                    out = cv2.resize(cropped, (300, 300)) #Resize face so all images have same size\n",
    "                    cv2.imwrite(\"../Datasets/A2_dataset_DNN/%s/%s.jpg\" %(emotion, filenumber), out) #Write image - don' need to worry about keeping track of labels associated because already sorted\n",
    "                    filenumber += 1 #Increment image number\n",
    "                except:\n",
    "                    pass #If error, pass file\n",
    "\n",
    "def detect_faces(emotion):\n",
    "    files = glob.glob(\"../Datasets/sorted_sets/%s/*.jpg\" %emotion) #Get list of all images with emotion\n",
    "    filenumber = 0\n",
    "    for f in files:\n",
    "        frame = cv2.imread(f) #Open image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #Convert image to grayscale\n",
    "        #Detect face using 4 different classifiers\n",
    "        face = faceDet.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face_two = faceDet_two.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face_three = faceDet_three.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face_four = faceDet_four.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        #Go over detected faces, stop at first detected face, return empty if no face.\n",
    "        if len(face) == 1:\n",
    "            facefeatures = face\n",
    "        elif len(face_two) == 1:\n",
    "            facefeatures = face_two\n",
    "        elif len(face_three) == 1:\n",
    "            facefeatures = face_three\n",
    "        elif len(face_four) == 1:\n",
    "            facefeatures = face_four\n",
    "        else:\n",
    "            facefeatures = \"\"\n",
    "        #Cut and save face\n",
    "        for (x, y, w, h) in facefeatures: #get coordinates and size of rectangle containing face\n",
    "            #print (\"face found in file: %s\" %f)\n",
    "            gray = gray[y:y+h, x:x+w] #Cut the frame to size\n",
    "            try:\n",
    "                out = cv2.resize(gray, (350, 350)) #Resize face so all images have same size\n",
    "                cv2.imwrite(\"../Datasets/A2_dataset/%s/%s.jpg\" %(emotion, filenumber), out) #Write image - don' need to worry about keeping track of labels associated because already sorted\n",
    "            except:\n",
    "                pass #If error, pass file\n",
    "        filenumber += 1 #Increment image number\n",
    "for emotion in emotions:\n",
    "    DNN_Face_Detection(emotion) #Call functiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromCaffe(\"../deep-learning-face-detection/deploy.prototxt.txt\", \"../deep-learning-face-detection/res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "\n",
    "def face_rect_DNN(image):\n",
    "    (h, w) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    \n",
    "    actual_detections = detections[0][0]\n",
    "    detection_accuracies = detections[0][0][:,2]\n",
    "    \n",
    "    index_values = np.argsort(detection_accuracies)\n",
    "    index_values = index_values[::-1]\n",
    "\n",
    "    sorted_detections = [actual_detections[i] for i in index_values]\n",
    "    sorted_detections = np.asarray(sorted_detections)\n",
    "\n",
    "    confidence = sorted_detections[0, 2]\n",
    "\n",
    "    # filter out weak detections by ensuring the `confidence` is\n",
    "    # greater than the minimum confidence\n",
    "    if confidence > 0.95:\n",
    "        # compute the (x, y)-coordinates of the bounding box for the\n",
    "        # object\n",
    "        box = sorted_detections[0, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        faceBoxRectangleS = dlib.rectangle(left=startX, top=startY, right=endX, bottom=endY)\n",
    "        return faceBoxRectangleS\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making set\n",
      " working on smiling\n",
      " working on not_smiling\n",
      "training SVM\n",
      "getting accuracy\n",
      "Accuracy:  0.8693820224719101\n"
     ]
    }
   ],
   "source": [
    "# Emotion classisification using facial landmarks\n",
    "\n",
    "emotions = [\"smiling\", \"not_smiling\"] #Define emotions\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\") #Or set this to whatever you named the downloaded file\n",
    "\n",
    "clf1 = SVC(kernel='rbf', gamma= 0.001, C= 10, decision_function_shape= 'OVO' ,probability=True)#, verbose = True) #Set the classifier as a support vector machine\n",
    "clf2 = SVC(kernel='rbf', gamma= 'scale', C = 10, decision_function_shape= 'OVO' ,probability=True)#, verbose = True) #Set the classifier as a support vector machine\n",
    "\n",
    "data = {} #Make dictionary for all values\n",
    "\n",
    "#data['landmarks_vectorised'] = []\n",
    "def get_files(emotion): #Define function to get file list, randomly shuffle it and split 80/20\n",
    "    files = glob.glob(\"../Datasets/A2_dataset_DNN/%s/*.jpg\" %emotion)\n",
    "    random.shuffle(files)\n",
    "    training = files[:int(len(files)*0.8)] #get first 80% of file list\n",
    "    prediction = files[-int(len(files)*0.2):] #get last 20% of file list\n",
    "    return training, prediction\n",
    "\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10,100,1000]\n",
    "    gammas = [0.00001, 0.0001, 0.001, 0.01, 0.1,1,10,'scale']\n",
    "    decision_function_shapes = ['OVO', 'OVA']\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas, 'decision_function_shape': decision_function_shapes}\n",
    "    grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    best_params = grid_search.best_params_\n",
    "    mean_CV_score = grid_search.cv_results_['mean_test_score']\n",
    "    return best_params, mean_CV_score\n",
    "\n",
    "def get_landmarks(image):\n",
    "    detections = detector(image, 1)\n",
    "    for k,d in enumerate(detections): #For all detected face instances individually\n",
    "        shape = predictor(image, d) #Draw Facial Landmarks with the predictor class\n",
    "        xlist = []\n",
    "        ylist = []\n",
    "        for i in range(1,68): #Store X and Y coordinates in two lists\n",
    "            xlist.append(float(shape.part(i).x))\n",
    "            ylist.append(float(shape.part(i).y))\n",
    "        xmean = np.mean(xlist)\n",
    "        ymean = np.mean(ylist)\n",
    "        xcentral = [(x-xmean) for x in xlist]\n",
    "        ycentral = [(y-ymean) for y in ylist]\n",
    "        landmarks_vectorised = []\n",
    "        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\n",
    "            landmarks_vectorised.append(w)\n",
    "            landmarks_vectorised.append(z)\n",
    "            meannp = np.asarray((ymean,xmean))\n",
    "            coornp = np.asarray((z,w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            landmarks_vectorised.append(dist)\n",
    "            landmarks_vectorised.append((math.atan2(y, x)*360)/(2*math.pi))\n",
    "        data['landmarks_vectorised'] = landmarks_vectorised\n",
    "    if len(detections) < 1:\n",
    "        data['landmarks_vectorised'] = \"error\"\n",
    "        \n",
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "    for emotion in emotions:\n",
    "        print(\" working on %s\" %emotion)\n",
    "        training, prediction = get_files(emotion)\n",
    "        #Append data to training and prediction list, and generate labels 0-1\n",
    "        count = 1;\n",
    "        for item in training:\n",
    "            image = cv2.imread(item) #open image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "            clahe_image = clahe.apply(gray)\n",
    "            get_landmarks(gray)\n",
    "            if data['landmarks_vectorised'] == \"error\":\n",
    "                count +=1\n",
    "            else:\n",
    "                training_data.append(data['landmarks_vectorised']) #append image array to training data list\n",
    "                training_labels.append(emotions.index(emotion))\n",
    "        count = 1;\n",
    "        for item in prediction:\n",
    "            image = cv2.imread(item)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            clahe_image = clahe.apply(gray)\n",
    "            get_landmarks(gray)\n",
    "            if data['landmarks_vectorised'] == \"error\":\n",
    "                count +=1\n",
    "            else:\n",
    "                prediction_data.append(data['landmarks_vectorised'])\n",
    "                prediction_labels.append(emotions.index(emotion))\n",
    "    return training_data, training_labels, prediction_data, prediction_labels\n",
    "\n",
    "print(\"Making set\")\n",
    "training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "npar_train = np.array(training_data) #Turn the training set into a numpy array for the classifier\n",
    "npar_trainlabs = np.array(training_labels)\n",
    "npar_pred = np.array(prediction_data)\n",
    "\n",
    "#best_params, mean_CV_score = svc_param_selection(npar_train,npar_trainlabs,5)\n",
    "#print(best_params)\n",
    "#plt.scatter(mean_CV_score)\n",
    "\n",
    "print(\"training SVM\") #train SVM\n",
    "clf2.fit(npar_train, npar_trainlabs)\n",
    "print(\"getting accuracy\") #Use score() function to get accuracy\n",
    "pred_lin = clf2.score(npar_pred, prediction_labels)\n",
    "print (\"Accuracy: \", pred_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'decision_function_shape': 'OVO', 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "best_params, mean_CV_score = svc_param_selection(npar_train,npar_trainlabs,5)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'decision_function_shape': 'OVO', 'gamma': 0.001}\n",
      "(84,)\n",
      "[[0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.74904361]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.74904361]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.75478194]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.75478194]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.75478194]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.75478194]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.75478194]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.75478194]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]\n",
      " [0.50076511]]\n"
     ]
    }
   ],
   "source": [
    "print(best_params)\n",
    "print(mean_CV_score.shape)\n",
    "print(mean_CV_score.reshape(mean_CV_score.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\herrn\\Documents\\University\\Anaconda\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\herrn\\Documents\\University\\Anaconda\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\herrn\\Documents\\University\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 600, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\herrn\\Documents\\University\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\herrn\\Documents\\University\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\herrn\\Documents\\University\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 516, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\herrn\\Documents\\University\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py\", line 147, in fit\n    y = self._validate_targets(y)\n  File \"C:\\Users\\herrn\\Documents\\University\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py\", line 521, in _validate_targets\n    \" class\" % len(cls))\nValueError: The number of classes has to be greater than one; got 1 class\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-e96cd7da5c68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rbf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m plt = plot_learning_curve(estimator, title, X, y, ylim=(0.75, 1.01),\n\u001b[1;32m--> 105\u001b[1;33m                     cv=kf, n_jobs=4)\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;31m#plt.savefig('../plots_A1/10_learningcurve.png')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-e96cd7da5c68>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[1;34m(estimator, title, X, y, axes, ylim, cv, n_jobs, train_sizes)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n\u001b[1;32m---> 66\u001b[1;33m                        train_sizes=train_sizes)\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mtrain_scores_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score)\u001b[0m\n\u001b[0;32m   1294\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m             error_score=error_score)\n\u001b[1;32m-> 1296\u001b[1;33m             for train, test in train_test_proportions)\n\u001b[0m\u001b[0;32m   1297\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m         \u001b[0mn_cv_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mn_unique_ticks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    552\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    553\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\Anaconda\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\University\\Anaconda\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    plt.title(title)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    print(train_sizes)\n",
    "    # Plot learning curve\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "X, y = npar_train, npar_trainlabs\n",
    "\n",
    "title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.01, C = 10, decision function shape = OVO$)\"\n",
    "\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "c= 10\n",
    "gamma = 0.001\n",
    "method = 'OVO'\n",
    "\n",
    "estimator = SVC(kernel='rbf',C = c , gamma = gamma, decision_function_shape = method)\n",
    "plt = plot_learning_curve(estimator, title, X, y, ylim=(0.75, 1.01),\n",
    "                    cv=kf, n_jobs=4)\n",
    "#plt.savefig('../plots_A1/10_learningcurve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
