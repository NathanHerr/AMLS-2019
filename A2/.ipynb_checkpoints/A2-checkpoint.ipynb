{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from natsort import natsorted, ns\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from shutil import copyfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of image names and corresponding gender classifications\n",
    "image_dic = pd.read_excel('../Datasets/labels_A.xlsx')\n",
    "image_dic = image_dic[['img_name.jpg', 'smiling']] # Choose columns which are of importance\n",
    "df = pd.DataFrame(image_dic)\n",
    "df.to_excel('../Datasets/source_emotions/labels_A2.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate smiling and not_smiling images and corresponding labels into folders\n",
    "\n",
    "source_emotions = pd.read_excel('../Datasets/source_emotions/labels_A2.xlsx')\n",
    "source_images_file_paths = glob.glob (\"../Datasets/source_images/*.jpg\") #find all paths which match the given path\n",
    "source_images_file_paths = natsorted(source_images_file_paths) #sort the list of file names such that the image list will be in the correct order\n",
    "\n",
    "smiling_images = []\n",
    "not_smiling_images = []\n",
    "\n",
    "smiling_directory = \"../Datasets/sorted_sets/smiling/\"\n",
    "not_smiling_directory = \"../Datasets/sorted_sets/not_smiling/\"\n",
    "\n",
    "for file_path in source_images_file_paths:\n",
    "    image = cv2.imread(file_path, cv2.COLOR_RGB2BGR) #read the image\n",
    "    image_name = os.path.basename(file_path)\n",
    "    image_label = source_emotions[source_emotions['img_name.jpg']==image_name]['smiling'].iloc[0]\n",
    "    if(image_label == 1):\n",
    "        smiling_images.append(image)\n",
    "        directory = ''.join([smiling_directory,os.path.basename(image_name)])\n",
    "        cv2.imwrite(directory, image)\n",
    "    else:\n",
    "        not_smiling_images.append(image)\n",
    "        directory = ''.join([not_smiling_directory,os.path.basename(image_name)])\n",
    "        cv2.imwrite(directory, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deetcting faces and cropping\n",
    "\n",
    "faceDet = cv2.CascadeClassifier(\"../OpenCV_FaceCascade/haarcascade_frontalface_default.xml\")\n",
    "faceDet_two = cv2.CascadeClassifier(\"../OpenCV_FaceCascade/haarcascade_frontalface_alt2.xml\")\n",
    "faceDet_three = cv2.CascadeClassifier(\"../OpenCV_FaceCascade/haarcascade_frontalface_alt.xml\")\n",
    "faceDet_four = cv2.CascadeClassifier(\"../OpenCV_FaceCascade/haarcascade_frontalface_alt_tree.xml\")\n",
    "emotions = [\"smiling\", \"not_smiling\"] #Define emotions\n",
    "def detect_faces(emotion):\n",
    "    files = glob.glob(\"../Datasets/sorted_sets/%s/*.jpg\" %emotion) #Get list of all images with emotion\n",
    "    filenumber = 0\n",
    "    for f in files:\n",
    "        frame = cv2.imread(f) #Open image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #Convert image to grayscale\n",
    "        #Detect face using 4 different classifiers\n",
    "        face = faceDet.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face_two = faceDet_two.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face_three = faceDet_three.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face_four = faceDet_four.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        #Go over detected faces, stop at first detected face, return empty if no face.\n",
    "        if len(face) == 1:\n",
    "            facefeatures = face\n",
    "        elif len(face_two) == 1:\n",
    "            facefeatures = face_two\n",
    "        elif len(face_three) == 1:\n",
    "            facefeatures = face_three\n",
    "        elif len(face_four) == 1:\n",
    "            facefeatures = face_four\n",
    "        else:\n",
    "            facefeatures = \"\"\n",
    "        #Cut and save face\n",
    "        for (x, y, w, h) in facefeatures: #get coordinates and size of rectangle containing face\n",
    "            #print (\"face found in file: %s\" %f)\n",
    "            gray = gray[y:y+h, x:x+w] #Cut the frame to size\n",
    "            try:\n",
    "                out = cv2.resize(gray, (350, 350)) #Resize face so all images have same size\n",
    "                cv2.imwrite(\"../Datasets/A2_dataset/%s/%s.jpg\" %(emotion, filenumber), out) #Write image - don' need to worry about keeping track of labels associated because already sorted\n",
    "            except:\n",
    "                pass #If error, pass file\n",
    "        filenumber += 1 #Increment image number\n",
    "for emotion in emotions:\n",
    "    detect_faces(emotion) #Call functiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion detection using fisher face and LBPH  \n",
    "\n",
    "emotions = [\"smiling\", \"not_smiling\"] #Define emotions\n",
    "\n",
    "fishface = cv2.face.FisherFaceRecognizer_create() #Initialize fisher face classifier\n",
    "radius = 1    \n",
    "no_neighbours = 8\n",
    "LBPH = cv2.face.LBPHFaceRecognizer_create(radius, no_neighbours)\n",
    "\n",
    "def get_files(emotion): #Define function to get file list, randomly shuffle it and split 80/20\n",
    "    files = glob.glob(\"../Datasets/A2_dataset/%s/*.jpg\" %emotion)\n",
    "    return files\n",
    "\n",
    "def make_sets():\n",
    "    image_inputs = []\n",
    "    image_labels = []\n",
    "    for emotion in emotions:\n",
    "        image_files = get_files(emotion)\n",
    "        #Append data and generate labels 0-1\n",
    "        for item in image_files:\n",
    "            image = cv2.imread(item) #open image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "            image_inputs.append(gray) #append image array to training data list\n",
    "            image_labels.append(emotions.index(emotion))\n",
    "            \n",
    "    training_data, prediction_data, training_labels, prediction_labels = train_test_split(image_inputs,image_labels)\n",
    "    print(len(training_data))\n",
    "    print(len(training_labels))\n",
    "    print(len(prediction_data))\n",
    "    print(len(prediction_labels))\n",
    "    return training_data, training_labels, prediction_data, prediction_labels\n",
    "\n",
    "def train_model(training_data, training_labels):\n",
    "    print (\"training fisher face classifier\")\n",
    "    print (\"size of training set is:\", len(training_labels), \"images\")\n",
    "    LBPH.train(training_data, np.asarray(training_labels))\n",
    "\n",
    "def run_classifier(prediction_data, prediction_labels):\n",
    "    print (\"predicting classification set\")\n",
    "    cnt = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    pred_list = []\n",
    "    for image in prediction_data:\n",
    "        pred, conf = LBPH.predict(image)\n",
    "        pred_list.append(pred)\n",
    "\n",
    "    accuracy = accuracy_score(prediction_labels, pred_list)\n",
    "    return accuracy\n",
    "\n",
    "# Main\n",
    "training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "train_model(training_data, training_labels)\n",
    "correct = run_classifier(prediction_data, prediction_labels)\n",
    "print (\"got\", correct, \"percent correct!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion classisification using facial landmarks\n",
    "\n",
    "emotions = [\"smiling\", \"not_smiling\"] #Define emotions\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\") #Or set this to whatever you named the downloaded file\n",
    "\n",
    "clf = SVC(kernel='rbf', gamma='scale', probability=True)#, verbose = True) #Set the classifier as a support vector machine\n",
    "data = {} #Make dictionary for all values\n",
    "\n",
    "#data['landmarks_vectorised'] = []\n",
    "def get_files(emotion): #Define function to get file list, randomly shuffle it and split 80/20\n",
    "    files = glob.glob(\"../Datasets/A2_dataset/%s/*.jpg\" %emotion)\n",
    "    random.shuffle(files)\n",
    "    training = files[:int(len(files)*0.8)] #get first 80% of file list\n",
    "    prediction = files[-int(len(files)*0.2):] #get last 20% of file list\n",
    "    return training, prediction\n",
    "\n",
    "def get_landmarks(image):\n",
    "    detections = detector(image, 1)\n",
    "    for k,d in enumerate(detections): #For all detected face instances individually\n",
    "        shape = predictor(image, d) #Draw Facial Landmarks with the predictor class\n",
    "        xlist = []\n",
    "        ylist = []\n",
    "        for i in range(1,68): #Store X and Y coordinates in two lists\n",
    "            xlist.append(float(shape.part(i).x))\n",
    "            ylist.append(float(shape.part(i).y))\n",
    "        xmean = np.mean(xlist)\n",
    "        ymean = np.mean(ylist)\n",
    "        xcentral = [(x-xmean) for x in xlist]\n",
    "        ycentral = [(y-ymean) for y in ylist]\n",
    "        landmarks_vectorised = []\n",
    "        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\n",
    "            landmarks_vectorised.append(w)\n",
    "            landmarks_vectorised.append(z)\n",
    "            meannp = np.asarray((ymean,xmean))\n",
    "            coornp = np.asarray((z,w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            landmarks_vectorised.append(dist)\n",
    "            landmarks_vectorised.append((math.atan2(y, x)*360)/(2*math.pi))\n",
    "        data['landmarks_vectorised'] = landmarks_vectorised\n",
    "    if len(detections) < 1:\n",
    "        data['landmarks_vestorised'] = \"error\"\n",
    "        \n",
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "    for emotion in emotions:\n",
    "        print(\" working on %s\" %emotion)\n",
    "        training, prediction = get_files(emotion)\n",
    "        #Append data to training and prediction list, and generate labels 0-1\n",
    "        for item in training:\n",
    "            image = cv2.imread(item) #open image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "            clahe_image = clahe.apply(gray)\n",
    "            get_landmarks(gray)\n",
    "            if data['landmarks_vectorised'] == \"error\":\n",
    "                print(\"no face detected on this one\")\n",
    "            else:\n",
    "                training_data.append(data['landmarks_vectorised']) #append image array to training data list\n",
    "                training_labels.append(emotions.index(emotion))\n",
    "        for item in prediction:\n",
    "            image = cv2.imread(item)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            clahe_image = clahe.apply(gray)\n",
    "            get_landmarks(clahe_image)\n",
    "            if data['landmarks_vectorised'] == \"error\":\n",
    "                print(\"no face detected on this one\")\n",
    "            else:\n",
    "                prediction_data.append(data['landmarks_vectorised'])\n",
    "                prediction_labels.append(emotions.index(emotion))\n",
    "    return training_data, training_labels, prediction_data, prediction_labels\n",
    "\n",
    "print(\"Making set\")\n",
    "training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "npar_train = np.array(training_data) #Turn the training set into a numpy array for the classifier\n",
    "npar_trainlabs = np.array(training_labels)\n",
    "print(training_labels[:10])\n",
    "print(\"training SVM linear\") #train SVM\n",
    "clf.fit(npar_train, npar_trainlabs)\n",
    "print(\"getting accuracy\") #Use score() function to get accuracy\n",
    "npar_pred = np.array(prediction_data)\n",
    "pred_lin = clf.score(npar_pred, prediction_labels)\n",
    "print (\"Accuracy: \", pred_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
